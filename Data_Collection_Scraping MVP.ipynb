{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c54d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b1e3b-7185-4fd6-98ea-bd54ab4f51cb",
   "metadata": {},
   "source": [
    "## **MVP Record (1991-2025)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5509208b-70f2-4eab-8b02-85d8a1aa1e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1990...\n",
      "Downloading 1991...\n",
      "Downloading 1992...\n",
      "Downloading 1993...\n",
      "Downloading 1994...\n",
      "Downloading 1995...\n",
      "Downloading 1996...\n",
      "Downloading 1997...\n",
      "Downloading 1998...\n",
      "Downloading 1999...\n",
      "Downloading 2000...\n",
      "Downloading 2001...\n",
      "Downloading 2002...\n",
      "Downloading 2003...\n",
      "Downloading 2004...\n",
      "Downloading 2005...\n",
      "Downloading 2006...\n",
      "Downloading 2007...\n",
      "Downloading 2008...\n",
      "Downloading 2009...\n",
      "Downloading 2010...\n",
      "Downloading 2011...\n",
      "Downloading 2012...\n",
      "Downloading 2013...\n",
      "Downloading 2014...\n",
      "Downloading 2015...\n",
      "Downloading 2016...\n",
      "Downloading 2017...\n",
      "Downloading 2018...\n",
      "Downloading 2019...\n",
      "Downloading 2020...\n",
      "Downloading 2021...\n",
      "Downloading 2022...\n",
      "Downloading 2023...\n",
      "Downloading 2024...\n",
      "Downloading 2025...\n",
      "Downloading 2026...\n",
      "Download complete.\n",
      "⚠️ No MVP table found for year 2026\n",
      "Parsing complete.\n"
     ]
    }
   ],
   "source": [
    "def scrape_mvp_awards(start_year=1990, end_year=2027, sleep=True):\n",
    "    years = list(range(start_year, end_year))\n",
    "    url_start = \"https://www.basketball-reference.com/awards/awards_{}.html\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\"\n",
    "    }\n",
    "\n",
    "    # STEP 1 — DOWNLOAD HTML FILES\n",
    "    for year in years:\n",
    "        if sleep:\n",
    "            time.sleep(random.uniform(2.5, 5.0))  # avoid rate limits\n",
    "\n",
    "        url = url_start.format(year)\n",
    "        print(f\"Downloading MVP page for {year}...\")\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        with open(f\"mvp/{year}.html\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    print(\"MVP HTML download complete.\")\n",
    "\n",
    "    # STEP 2 — PARSE MVP HTML FILES\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        with open(f\"mvp/{year}.html\", encoding=\"utf-8\") as f:\n",
    "            page = f.read()\n",
    "\n",
    "        if \"Rate Limited Request\" in page:\n",
    "            print(f\" {year} blocked (429). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "        # remove duplicate header row\n",
    "        over_header = soup.find(\"tr\", class_=\"over_header\")\n",
    "        if over_header:\n",
    "            over_header.decompose()\n",
    "\n",
    "        table = soup.find(id=\"mvp\")\n",
    "        if table is None:\n",
    "            print(f\" No MVP table found for {year}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_html(StringIO(str(table)))[0]\n",
    "        df[\"Year\"] = year\n",
    "        dfs.append(df)\n",
    "\n",
    "    print(\"MVP parsing complete.\")\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7fd04-694f-4c1c-97c1-da426fd940e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1990, 2027))\n",
    "mvp_df = scrape_team_standings(years)\n",
    "\n",
    "print(mvp_df.head())\n",
    "print(mvp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535cb733-bb4e-4bd0-a9ef-cf18fb27c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvp_df.to_csv(\"mvps.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9fa97-7dd1-441a-aac3-de3efbeb2447",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fbbce9",
   "metadata": {},
   "source": [
    "## **Players Record (1991-2025)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b756299-0c5d-4b56-9449-56444e8ba850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_player_stats(driver, start_year=1990, end_year=2027, sleep=True):\n",
    "    years = list(range(start_year, end_year))\n",
    "    url_template = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\"\n",
    "\n",
    "    # STEP 1 — Download HTML using Selenium\n",
    "    for year in years:\n",
    "        url = url_template.format(year)\n",
    "        print(f\"Downloading player stats for {year}: {url}\")\n",
    "\n",
    "        driver.get(url)\n",
    "        driver.execute_script(\"window.scrollTo(1, 10000)\")\n",
    "\n",
    "        if sleep:\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "\n",
    "        html = driver.page_source\n",
    "        with open(f\"player/{year}.html\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(html)\n",
    "\n",
    "    print(\"Player HTML download complete.\")\n",
    "\n",
    "    # STEP 2 — Parse HTML\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        with open(f\"player/{year}.html\", encoding=\"utf-8\") as f:\n",
    "            page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "        # Remove duplicate table header\n",
    "        thead = soup.find(\"tr\", class_=\"thead\")\n",
    "        if thead:\n",
    "            thead.decompose()\n",
    "\n",
    "        table = soup.find(id=\"per_game_stats\")\n",
    "        if table is None:\n",
    "            print(f\" No player stats table for {year}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_html(StringIO(str(table)))[0]\n",
    "        df[\"Year\"] = year\n",
    "        dfs.append(df)\n",
    "\n",
    "    print(\"Player stats parsing complete.\")\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce894d-9a67-4a0f-801a-7197b564b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1990, 2027))\n",
    "player_df = scrape_team_standings(years)\n",
    "\n",
    "print(player_df.head())\n",
    "print(player_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a967962-087e-4c39-b8af-5136e2661757",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_player_df.to_csv(\"players.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e9f1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39328b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xattr -d com.apple.quarantine chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d53a9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16694621-9ffa-4ba1-be41-24fccf8eb516",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb9405",
   "metadata": {},
   "source": [
    "## **Teams Record (1991-2025)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9726b225-bb55-4cdb-a80a-7e1cea98583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_team_standings(years, sleep=True):\n",
    "    team_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\"\n",
    "    \n",
    "    # ------------------------\n",
    "    # STEP 1 — Download HTMLs\n",
    "    # ------------------------\n",
    "    for year in years:\n",
    "        url = team_stats_url.format(year)\n",
    "\n",
    "        if sleep:\n",
    "            time.sleep(random.uniform(2.5, 5.0))\n",
    "\n",
    "        print(f\"Downloading {year}...\")\n",
    "\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "\n",
    "        with open(f\"team/{year}.html\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(html)\n",
    "\n",
    "    print(\"Team HTML download complete.\")\n",
    "\n",
    "    # ------------------------\n",
    "    # STEP 2 — Parse Each File\n",
    "    # ------------------------\n",
    "    dfs = []\n",
    "\n",
    "    for year in years:\n",
    "        with open(f\"team/{year}.html\", encoding=\"utf-8\") as f:\n",
    "            page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "        # Remove extra header row if present\n",
    "        thead = soup.find(\"tr\", class_=\"thead\")\n",
    "        if thead:\n",
    "            thead.decompose()\n",
    "\n",
    "        # ------------------------\n",
    "        # EASTERN CONFERENCE\n",
    "        # ------------------------\n",
    "        east_table = soup.find(id=\"divs_standings_E\")\n",
    "        if east_table is not None:\n",
    "            east_df = pd.read_html(StringIO(str(east_table)))[0]\n",
    "            east_df[\"Year\"] = year\n",
    "            east_df[\"Team\"] = east_df[\"Eastern Conference\"]\n",
    "            del east_df[\"Eastern Conference\"]\n",
    "            dfs.append(east_df)\n",
    "        else:\n",
    "            print(f\" No East table found for {year}\")\n",
    "\n",
    "        # Reload soup to avoid modified DOM\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        thead = soup.find(\"tr\", class_=\"thead\")\n",
    "        if thead:\n",
    "            thead.decompose()\n",
    "\n",
    "        # ------------------------\n",
    "        # WESTERN CONFERENCE\n",
    "        # ------------------------\n",
    "        west_table = soup.find(id=\"divs_standings_W\")\n",
    "        if west_table is not None:\n",
    "            west_df = pd.read_html(StringIO(str(west_table)))[0]\n",
    "            west_df[\"Year\"] = year\n",
    "            west_df[\"Team\"] = west_df[\"Western Conference\"]\n",
    "            del west_df[\"Western Conference\"]\n",
    "            dfs.append(west_df)\n",
    "        else:\n",
    "            print(f\" No West table found for {year}\")\n",
    "\n",
    "    print(\"Team data parsing complete.\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae13369-8e2a-4ca0-9b15-9d212c2ce555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1990...\n",
      "Downloading 1991...\n",
      "Downloading 1992...\n",
      "Downloading 1993...\n",
      "Downloading 1994...\n",
      "Downloading 1995...\n",
      "Downloading 1996...\n",
      "Downloading 1997...\n",
      "Downloading 1998...\n",
      "Downloading 1999...\n",
      "Downloading 2000...\n",
      "Downloading 2001...\n",
      "Downloading 2002...\n",
      "Downloading 2003...\n",
      "Downloading 2004...\n",
      "Downloading 2005...\n",
      "Downloading 2006...\n",
      "Downloading 2007...\n",
      "Downloading 2008...\n",
      "Downloading 2009...\n",
      "Downloading 2010...\n",
      "Downloading 2011...\n",
      "Downloading 2012...\n",
      "Downloading 2013...\n",
      "Downloading 2014...\n",
      "Downloading 2015...\n",
      "Downloading 2016...\n",
      "Downloading 2017...\n",
      "Downloading 2018...\n",
      "Downloading 2019...\n",
      "Downloading 2020...\n",
      "Downloading 2021...\n",
      "Downloading 2022...\n",
      "Downloading 2023...\n",
      "Downloading 2024...\n",
      "Downloading 2025...\n",
      "Downloading 2026...\n",
      "Team HTML download complete.\n",
      "Team data parsing complete.\n",
      "    W   L  W/L%    GB   PS/G   PA/G    SRS  Year                 Team\n",
      "0  53  29  .646     —  110.2  105.2   4.23  1990  Philadelphia 76ers*\n",
      "1  52  30  .634   1.0  110.0  106.0   3.23  1990      Boston Celtics*\n",
      "2  45  37  .549   8.0  108.3  106.9   0.78  1990     New York Knicks*\n",
      "3  31  51  .378  22.0  107.7  109.9  -2.43  1990   Washington Bullets\n",
      "4  18  64  .220  35.0  100.6  110.3  -9.59  1990           Miami Heat\n",
      "(1238, 9)\n"
     ]
    }
   ],
   "source": [
    "years = list(range(1990, 2027))\n",
    "team_df = scrape_team_standings(years)\n",
    "\n",
    "print(team_df.head())\n",
    "print(team_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd7f9f3-b1dd-4dd4-b47c-908dde76ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df.to_csv(\"teams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f8ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
